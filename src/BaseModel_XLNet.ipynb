{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import OrderedDict\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "#from apex import amp\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import logging\n",
    "from logger import logger\n",
    "logging.getLogger(\"ppb.tokenization_utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = int(os.environ.get(\"EPOCHS\", 10))\n",
    "TRAIN_BATCH_SIZE = int(os.environ.get(\"TRAIN_BATCH_SIZE\", 4))\n",
    "TEST_BATCH_SIZE = int(os.environ.get(\"TEST_BATCH_SIZE\", 4))\n",
    "BASE_MODEL = 'xlnet-large-cased'#'xlnet-base-cased'\n",
    "TRAINING_DATASET = os.environ.get(\"TRAINING_DATASET\", \"../input/google-quest-challenge/train_group_folds.csv\")\n",
    "TEST_DATASET = os.environ.get(\"TEST_DATASET\", \"../input/google-quest-challenge/test.csv\")\n",
    "SAMPLE_SUBMISSION = os.environ.get(\"SAMPLE_SUBMISSION\", \"../input/google-quest-challenge/sample_submission.csv\")\n",
    "ACCUMULATION_STEPS = int(os.environ.get(\"ACCUMULATION_STEPS\", 2))\n",
    "\n",
    "SEED = 19840916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\"):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
    "            torch.save(model.module.state_dict(), model_path)\n",
    "            #torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tag(text):\n",
    "    if '[math]' in text:\n",
    "        text = re.sub('\\[math\\].*?math\\]', '[formula]', text)\n",
    "    if 'http' in text or 'www' in text:\n",
    "        text = re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', '[url]', text)\n",
    "    #text = re.sub('<\\/?[^>]*>', '[tag]', text)\n",
    "    #text = re.sub('{\\/?[^>]*}', '[code]', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def convert_example_to_features(text1,\n",
    "                                text2,\n",
    "                                max_seq_length,\n",
    "                                tokenizer,\n",
    "                                cls_token_at_end=True,\n",
    "                                cls_token='[CLS]',\n",
    "                                cls_token_segment_id=1,\n",
    "                                sep_token='[SEP]',\n",
    "                                sep_token_extra=False,\n",
    "                                pad_on_left=False,\n",
    "                                pad_token=0,\n",
    "                                pad_token_segment_id=0,\n",
    "                                sequence_a_segment_id=0,\n",
    "                                sequence_b_segment_id=1,\n",
    "                                mask_padding_with_zero=True):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "    example = (text1, text2)\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example[0])\n",
    "\n",
    "    tokens_b = None\n",
    "    if example[1] is not None:\n",
    "        tokens_b = tokenizer.tokenize(example[1])\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\". \" -4\" for RoBERTa.\n",
    "        special_tokens_count = 4 if sep_token_extra else 3\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - special_tokens_count)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = 3 if sep_token_extra else 2\n",
    "        if len(tokens_a) > max_seq_length - special_tokens_count:\n",
    "            tokens_a = tokens_a[:(max_seq_length - special_tokens_count)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids:   0   0   0   0  0     0   0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    if sep_token_extra:\n",
    "        # roberta uses an extra separator b/w pairs of sentences\n",
    "        tokens += [sep_token]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    if tokens_b:\n",
    "        tokens += tokens_b + [sep_token]\n",
    "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "    if cls_token_at_end:\n",
    "        tokens = tokens + [cls_token]\n",
    "        segment_ids = segment_ids + [cls_token_segment_id]\n",
    "    else:\n",
    "        tokens = [cls_token] + tokens\n",
    "        segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "    else:\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTraining:\n",
    "    def __init__(self, qtitle, qbody, answer, targets, tokenizer, max_length, category=None):\n",
    "        self.qtitle = qtitle\n",
    "        self.qbody = qbody\n",
    "        self.answer = answer\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.category = category\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answer)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        question_title = str(self.qtitle[item])\n",
    "        question_body = str(self.qbody[item])\n",
    "        answer_text = str(self.answer[item])\n",
    "\n",
    "        ques_ids, ques_mask, ques_token_type_ids = convert_example_to_features(\n",
    "            text1=question_title,\n",
    "            text2=question_body,\n",
    "            max_seq_length=self.max_length,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        if self.category is not None:\n",
    "            category_text = str(self.category[item])\n",
    "            answer_ids, answer_mask, answer_token_type_ids = convert_example_to_features(\n",
    "                text1=answer_text,\n",
    "                text2=category_text,\n",
    "                max_seq_length=self.max_length,\n",
    "                tokenizer=self.tokenizer\n",
    "            )\n",
    "        else:\n",
    "            answer_ids, answer_mask, answer_token_type_ids = convert_example_to_features(\n",
    "                text1=answer_text,\n",
    "                text2=None,\n",
    "                max_seq_length=self.max_length,\n",
    "                tokenizer=self.tokenizer\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            'question_ids': torch.tensor(ques_ids, dtype=torch.long),\n",
    "            'question_mask': torch.tensor(ques_mask, dtype=torch.long),\n",
    "            'question_token_type_ids': torch.tensor(ques_token_type_ids, dtype=torch.long),\n",
    "            'answer_ids': torch.tensor(answer_ids, dtype=torch.long),\n",
    "            'answer_mask': torch.tensor(answer_mask, dtype=torch.long),\n",
    "            'answer_token_type_ids': torch.tensor(answer_token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[item, :], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "\n",
    "class DatasetTest:\n",
    "    def __init__(self, qtitle, qbody, answer, tokenizer, max_length, category=None):\n",
    "        self.qtitle = qtitle\n",
    "        self.qbody = qbody\n",
    "        self.answer = answer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.category = category\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answer)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        question_title = str(self.qtitle[item])\n",
    "        question_body = str(self.qbody[item])\n",
    "        answer_text = str(self.answer[item])\n",
    "\n",
    "        if self.category is not None:\n",
    "            category_text = str(self.category[item])\n",
    "            answer_ids, answer_mask, answer_token_type_ids = convert_example_to_features(\n",
    "                text1=answer_text,\n",
    "                text2=category_text,\n",
    "                max_seq_length=self.max_length,\n",
    "                tokenizer=self.tokenizer\n",
    "            )\n",
    "        else:\n",
    "            answer_ids, answer_mask, answer_token_type_ids = convert_example_to_features(\n",
    "                text1=answer_text,\n",
    "                text2=None,\n",
    "                max_seq_length=self.max_length,\n",
    "                tokenizer=self.tokenizer\n",
    "            )\n",
    "\n",
    "        ques_ids, ques_mask, ques_token_type_ids = convert_example_to_features(\n",
    "            text1=question_title,\n",
    "            text2=question_body,\n",
    "            max_seq_length=self.max_length,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'question_ids': torch.tensor(ques_ids, dtype=torch.long),\n",
    "            'question_mask': torch.tensor(ques_mask, dtype=torch.long),\n",
    "            'question_token_type_ids': torch.tensor(ques_token_type_ids, dtype=torch.long),\n",
    "            'answer_ids': torch.tensor(answer_ids, dtype=torch.long),\n",
    "            'answer_mask': torch.tensor(answer_mask, dtype=torch.long),\n",
    "            'answer_token_type_ids': torch.tensor(answer_token_type_ids, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLNetBaseUncased, self).__init__()\n",
    "\n",
    "        self.pretrain = transformers.XLNetModel.from_pretrained(BASE_MODEL)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.hidden_size = 1024\n",
    "\n",
    "        self.out1 = nn.Linear(self.hidden_size, 21)\n",
    "        self.out2 = nn.Linear(self.hidden_size * 2, 9)\n",
    "\n",
    "        self.W_s1 = nn.Linear(self.hidden_size, 256, bias=True)\n",
    "        self.W_s2 = nn.Linear(256, 128, bias=True)\n",
    "\n",
    "    def attention_net(self, lstm_output):\n",
    "        \"\"\"\n",
    "        Now we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
    "        encoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of\n",
    "        the input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully\n",
    "        connected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e.,\n",
    "        pos & neg.\n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
    "        ---------\n",
    "        Returns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
    "                  attention to different parts of the input sentence.\n",
    "        Tensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "                      attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
    "        \"\"\"\n",
    "        attn_weight_matrix = self.W_s2(torch.tanh(self.W_s1(lstm_output)))  # / self.temper\n",
    "        attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
    "        attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
    "\n",
    "        return attn_weight_matrix\n",
    "\n",
    "    def forward(self,\n",
    "                question_ids,\n",
    "                question_mask,\n",
    "                question_token_type_ids,\n",
    "                answer_ids,\n",
    "                answer_mask,\n",
    "                answer_token_type_ids,\n",
    "                labels=None\n",
    "                ):\n",
    "        q_out = self.pretrain(question_ids, attention_mask=question_mask)[0]\n",
    "        q_weight_matrix = self.attention_net(q_out)\n",
    "        q_out = torch.matmul(q_weight_matrix, q_out)\n",
    "        q_out = q_out[:, 0, :]\n",
    "        q_out = self.drop(q_out)\n",
    "\n",
    "        a_out = self.pretrain(answer_ids, attention_mask=answer_mask)[0]\n",
    "        a_weight_matrix = self.attention_net(a_out)\n",
    "        a_out = torch.matmul(a_weight_matrix, a_out)\n",
    "        a_out = a_out[:, 0, :]\n",
    "        a_out = self.drop(a_out)\n",
    "\n",
    "        final1 = self.out1(q_out)\n",
    "        final2 = self.out2(torch.cat((q_out, a_out), 1))\n",
    "\n",
    "        final = torch.cat([final1, final2], 1)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    class_loss = 0.\n",
    "    for i in range(30):\n",
    "        class_loss += nn.BCEWithLogitsLoss()(preds[:, i:i+1], labels[:, i:i+1])\n",
    "    return class_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, dataset, data_loader, model, optimizer, accumulation_steps):\n",
    "    losses = AverageMeter()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "\n",
    "    model.train()\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    tk0 = tqdm(data_loader, total=int(len(dataset) / data_loader.batch_size))\n",
    "    for bi, d in enumerate(tk0):\n",
    "        question_ids = d[\"question_ids\"]\n",
    "        question_mask = d[\"question_mask\"]\n",
    "        question_token_type_ids = d[\"question_token_type_ids\"]\n",
    "        answer_ids = d[\"answer_ids\"]\n",
    "        answer_mask = d[\"answer_mask\"]\n",
    "        answer_token_type_ids = d[\"answer_token_type_ids\"]\n",
    "        targets = d[\"targets\"]\n",
    "\n",
    "        question_ids = question_ids.to(DEVICE, dtype=torch.long)\n",
    "        question_mask = question_mask.to(DEVICE, dtype=torch.long)\n",
    "        question_token_type_ids = question_token_type_ids.to(DEVICE, dtype=torch.long)\n",
    "        answer_ids = answer_ids.to(DEVICE, dtype=torch.long)\n",
    "        answer_mask = answer_mask.to(DEVICE, dtype=torch.long)\n",
    "        answer_token_type_ids = answer_token_type_ids.to(DEVICE, dtype=torch.long)\n",
    "        targets = targets.to(DEVICE, dtype=torch.float)\n",
    "\n",
    "        outputs = model(\n",
    "            question_ids=question_ids,\n",
    "            question_mask=question_mask,\n",
    "            question_token_type_ids=question_token_type_ids,\n",
    "            answer_ids=answer_ids,\n",
    "            answer_mask=answer_mask,\n",
    "            answer_token_type_ids=answer_token_type_ids,\n",
    "            labels=targets\n",
    "        )\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        targets_np = targets.cpu().detach().numpy()\n",
    "        outputs_np = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "        fin_targets.append(targets_np)\n",
    "        fin_outputs.append(outputs_np)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (bi + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        losses.update(loss.item(), question_ids.size(0))\n",
    "        tk0.set_postfix(loss=losses.avg)\n",
    "\n",
    "    spear = 0\n",
    "    for i in range(30):\n",
    "        spear += np.nan_to_num(\n",
    "            spearmanr(np.concatenate(fin_targets)[:, i],\n",
    "                      np.concatenate(fin_outputs)[:, i]\n",
    "                     ).correlation / 30)\n",
    "\n",
    "    log = OrderedDict([\n",
    "        ('loss', losses.avg),\n",
    "        ('spearman', spear)\n",
    "    ])\n",
    "    tk0.close()\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, data_loader, model, df_train, sample):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=int(len(dataset) / data_loader.batch_size))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            question_ids = d[\"question_ids\"]\n",
    "            question_mask = d[\"question_mask\"]\n",
    "            question_token_type_ids = d[\"question_token_type_ids\"]\n",
    "            answer_ids = d[\"answer_ids\"]\n",
    "            answer_mask = d[\"answer_mask\"]\n",
    "            answer_token_type_ids = d[\"answer_token_type_ids\"]\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            question_ids = question_ids.to(DEVICE, dtype=torch.long)\n",
    "            question_mask = question_mask.to(DEVICE, dtype=torch.long)\n",
    "            question_token_type_ids = question_token_type_ids.to(DEVICE, dtype=torch.long)\n",
    "            answer_ids = answer_ids.to(DEVICE, dtype=torch.long)\n",
    "            answer_mask = answer_mask.to(DEVICE, dtype=torch.long)\n",
    "            answer_token_type_ids = answer_token_type_ids.to(DEVICE, dtype=torch.long)\n",
    "            targets = targets.to(DEVICE, dtype=torch.float)\n",
    "\n",
    "            outputs = model(\n",
    "                question_ids=question_ids,\n",
    "                question_mask=question_mask,\n",
    "                question_token_type_ids=question_token_type_ids,\n",
    "                answer_ids=answer_ids,\n",
    "                answer_mask=answer_mask,\n",
    "                answer_token_type_ids=answer_token_type_ids,\n",
    "                labels=targets\n",
    "            )\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            targets_np = targets.cpu().detach().numpy()\n",
    "            outputs_np = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "            fin_targets.append(targets_np)\n",
    "            fin_outputs.append(outputs_np)\n",
    "\n",
    "            losses.update(loss.item(), question_ids.size(0))\n",
    "            tk0.set_postfix(loss=losses.avg)\n",
    "\n",
    "    fin_outputs = np.vstack(fin_outputs)\n",
    "    fin_targets = np.vstack(fin_targets)\n",
    "    spear = []\n",
    "    for jj in range(fin_targets.shape[1]):\n",
    "        p1 = list(fin_targets[:, jj])\n",
    "        p2 = list(fin_outputs[:, jj])\n",
    "        coef, _ = stats.spearmanr(p1, p2)\n",
    "        coef = np.nan_to_num(coef)\n",
    "        spear.append(coef)\n",
    "    spear = np.mean(spear)\n",
    "\n",
    "    log = OrderedDict([\n",
    "        ('loss', losses.avg),\n",
    "        ('spearman', spear)\n",
    "    ])\n",
    "    tk0.close()\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    seed_torch()\n",
    "\n",
    "    df = pd.read_csv(TRAINING_DATASET).fillna(\"none\")\n",
    "    df_test = pd.read_csv(TEST_DATASET).fillna(\"none\")\n",
    "    sample = pd.read_csv(SAMPLE_SUBMISSION)\n",
    "\n",
    "    target_cols = list(sample.drop(\"qa_id\", axis=1).columns)\n",
    "    for col in target_cols:\n",
    "        u, c = np.unique(df[col].values, return_counts=True)\n",
    "        v = (np.cumsum(c) - c[0]) / c.sum()\n",
    "        mn = v.min()\n",
    "        mx = v.max()\n",
    "        v = (v - mn) / (mx - mn)\n",
    "        d = dict(zip(u, v))\n",
    "        df[col + '_ord'] = df[col].map(d)\n",
    "\n",
    "    target_cols_ord = [x + '_ord' for x in target_cols]\n",
    "    \n",
    "    df['question_title'] = df['question_title'].apply(clean_tag)\n",
    "    df['question_body'] = df['question_body'].apply(clean_tag)\n",
    "    df['answer'] = df['answer'].apply(clean_tag)\n",
    "\n",
    "    max_len = 512\n",
    "\n",
    "    tokenizer = transformers.XLNetTokenizer.from_pretrained(BASE_MODEL)\n",
    "    tokenizers = {\"tokenizer\": tokenizer, \"max_len\": max_len}\n",
    "    joblib.dump(tokenizers, f\"tokenizers_{BASE_MODEL}.pkl\")\n",
    "\n",
    "    NFOLDS = 5\n",
    "\n",
    "    CURRENT_FOLD = 0\n",
    "    for fold in range(NFOLDS):\n",
    "            \n",
    "        print('Start: ', fold, 'fold')\n",
    "        CURRENT_FOLD = fold\n",
    "        \n",
    "        df_train = df.loc[df.kfold != fold, :]\n",
    "        df_valid = df.loc[df.kfold == fold, :]\n",
    "        #train_targets = df_train[target_cols].values\n",
    "        train_targets = df_train[target_cols_ord].values\n",
    "        valid_targets = df_valid[target_cols].values\n",
    "\n",
    "        print(df_train.shape, df_valid.shape)\n",
    "\n",
    "        train_qtitle = df_train.question_title.values.astype(str).tolist()\n",
    "        train_qbody = df_train.question_body.values.astype(str).tolist()\n",
    "        train_answer = df_train.answer.values.astype(str).tolist()\n",
    "\n",
    "        valid_qtitle = df_valid.question_title.values.astype(str).tolist()\n",
    "        valid_qbody = df_valid.question_body.values.astype(str).tolist()\n",
    "        valid_answer = df_valid.answer.values.astype(str).tolist()\n",
    "\n",
    "        train_dataset = DatasetTraining(\n",
    "            qtitle=train_qtitle,\n",
    "            qbody=train_qbody,\n",
    "            answer=train_answer,\n",
    "            targets=train_targets,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=max_len\n",
    "        )\n",
    "        train_data_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        valid_dataset = DatasetTraining(\n",
    "            qtitle=valid_qtitle,\n",
    "            qbody=valid_qbody,\n",
    "            answer=valid_answer,\n",
    "            targets=valid_targets,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=max_len\n",
    "        )\n",
    "        valid_data_loader = torch.utils.data.DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=TEST_BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        model = XLNetBaseUncased()\n",
    "        model.to(DEVICE)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE / ACCUMULATION_STEPS * EPOCHS)\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=1.5e-5)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_train_steps\n",
    "        )\n",
    "\n",
    "        #model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\", verbosity=0)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            logger.info(\"Let's use %s GPUs!\" % torch.cuda.device_count())\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        es = EarlyStopping(patience=5, mode=\"max\")\n",
    "\n",
    "        logger.info(\"Training batch size: %s\" % TRAIN_BATCH_SIZE)\n",
    "        logger.info(\"Test batch size: %s\" % TEST_BATCH_SIZE)\n",
    "        logger.info(\"Epochs: %s\" % EPOCHS)\n",
    "        logger.info(\"Number of training samples: %s\" % len(train_dataset))\n",
    "        logger.info(\"Number of validation samples: %s\" % len(valid_dataset))\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            logger.info(\"Training Epoch: %s\" % epoch)\n",
    "            log = train(epoch, train_dataset, train_data_loader, model, optimizer, accumulation_steps=ACCUMULATION_STEPS)\n",
    "            logger.info(\"Validation Epoch: %s\" % epoch)\n",
    "            val_log = evaluate(valid_dataset, valid_data_loader, model, df_train, sample)\n",
    "            es(val_log[\"spearman\"], model, model_path=f\"./checkpoints/{BASE_MODEL}_{CURRENT_FOLD}.bin\")\n",
    "            if es.early_stop:\n",
    "                logger.info(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        del model, train_dataset, train_data_loader, valid_dataset, valid_data_loader, optimizer\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
